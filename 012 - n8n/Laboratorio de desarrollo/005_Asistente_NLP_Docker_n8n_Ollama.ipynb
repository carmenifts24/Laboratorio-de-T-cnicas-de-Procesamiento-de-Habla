{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bDXS2RmSFKo"
      },
      "source": [
        "# Procesamiento del Lenguaje Natural\n",
        "## Asistente NLP con Docker, n8n y Ollama\n",
        "\n",
        "**Materia:** Procesamiento del Lenguaje Natural e Introducción a LLMs  \n",
        "**Módulo:** Infraestructura y Despliegue de Aplicaciones NLP\n",
        "\n",
        "---\n",
        "\n",
        "## Introducción\n",
        "\n",
        "En esta guía vas a aprender a construir un sistema completo de procesamiento de lenguaje natural usando herramientas de código abierto. El objetivo es que entiendas cómo funcionan las aplicaciones de NLP en entornos reales, más allá de los notebooks de Jupyter.\n",
        "\n",
        "### ¿Qué vamos a construir?\n",
        "\n",
        "Un asistente de lenguaje natural que puede realizar múltiples tareas:\n",
        "\n",
        "- **Named Entity Recognition (NER):** Extracción de entidades nombradas\n",
        "- **Análisis de Sentimiento:** Clasificación emocional de textos\n",
        "- **Sumarización:** Generación de resúmenes automáticos\n",
        "- **Clasificación de Textos:** Identificación de tipo y tema\n",
        "- **Extracción de Keywords:** Términos más relevantes\n",
        "\n",
        "### Stack Tecnológico\n",
        "\n",
        "- **Docker + Docker Compose:** Containerización y orquestación\n",
        "- **n8n:** Plataforma de automatización de workflows\n",
        "- **Ollama:** Servidor para ejecutar modelos de lenguaje localmente\n",
        "- **IBM Granite 4:** Modelo de lenguaje de última generación\n",
        "- **Streamlit:** Framework para crear interfaces web (opcional)\n",
        "\n",
        "### Objetivos de Aprendizaje\n",
        "\n",
        "Al finalizar este laboratorio vas a poder:\n",
        "\n",
        "1. Comprender qué es Docker y por qué se usa en proyectos de IA\n",
        "2. Configurar y orquestar servicios con Docker Compose\n",
        "3. Trabajar con modelos de lenguaje locales sin depender de APIs externas\n",
        "4. Crear workflows de NLP de forma visual con n8n\n",
        "5. Implementar un asistente multimodal de procesamiento de lenguaje\n",
        "6. Crear imágenes Docker personalizadas\n",
        "7. Entender los fundamentos del despliegue en servidores\n",
        "\n",
        "### Nota Importante sobre el Entorno de Ejecución\n",
        "\n",
        "**Este proyecto NO puede ejecutarse en Google Colab** porque requiere Docker, que no está disponible en entornos Jupyter en la nube. Vas a necesitar trabajar en tu máquina local con Docker Desktop instalado.\n",
        "\n",
        "Este notebook funciona como una **guía de referencia** que vas a consultar mientras trabajás en tu terminal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN4ZHPKoSFKp"
      },
      "source": [
        "---\n",
        "\n",
        "## Conceptos Fundamentales\n",
        "\n",
        "Antes de comenzar con la implementación, es fundamental que entiendas los conceptos detrás de las tecnologías que vamos a usar.\n",
        "\n",
        "### Docker: Containerización de Aplicaciones\n",
        "\n",
        "Docker es una plataforma que permite empaquetar aplicaciones con todas sus dependencias en **contenedores**. Un contenedor es como una caja aislada que contiene:\n",
        "\n",
        "- El código de tu aplicación\n",
        "- Las librerías y dependencias necesarias\n",
        "- Configuraciones del sistema\n",
        "- Un mini sistema operativo base\n",
        "\n",
        "**¿Por qué es importante Docker?**\n",
        "\n",
        "El problema clásico en desarrollo de software es: \"en mi máquina funciona\". Docker lo resuelve garantizando que tu aplicación corra exactamente igual en cualquier lugar: tu laptop, el servidor de producción, la máquina de un colega.\n",
        "\n",
        "**Ventajas:**\n",
        "- **Portabilidad:** El mismo contenedor corre en Windows, Mac, Linux\n",
        "- **Reproducibilidad:** Mismo entorno siempre\n",
        "- **Aislamiento:** Cada servicio en su propio contenedor\n",
        "- **Eficiencia:** Los contenedores comparten el kernel del sistema operativo, son más livianos que máquinas virtuales\n",
        "\n",
        "### Docker Compose: Orquestación de Servicios\n",
        "\n",
        "Docker Compose es una herramienta para definir y ejecutar aplicaciones Docker con múltiples contenedores. En lugar de levantar cada contenedor manualmente, definís todos los servicios en un archivo YAML y los levantás con un solo comando.\n",
        "\n",
        "En nuestro proyecto vamos a orquestar dos servicios:\n",
        "- **n8n:** El motor de workflows\n",
        "- **Ollama:** El servidor de modelos de lenguaje\n",
        "\n",
        "### n8n: Automatización Visual de Workflows\n",
        "\n",
        "n8n (\"node-to-node\") es una plataforma open source de automatización que te permite crear flujos de trabajo conectando diferentes servicios. Es similar a herramientas como Zapier o Make, pero autohospedada y con más control.\n",
        "\n",
        "**¿Por qué usar n8n?**\n",
        "\n",
        "Podríamos escribir todo el código en Python, pero n8n nos da:\n",
        "- **Visualización:** Ves el flujo de datos gráficamente\n",
        "- **Debugging:** Podés ejecutar paso a paso y ver resultados intermedios\n",
        "- **Flexibilidad:** Fácil de modificar sin tocar código\n",
        "- **Integraciones:** Conectores pre-construidos para cientos de servicios\n",
        "\n",
        "En nuestro proyecto, n8n va a:\n",
        "1. Recibir consultas del usuario vía webhook\n",
        "2. Detectar qué tipo de tarea de NLP quiere realizar\n",
        "3. Construir el prompt apropiado para el modelo\n",
        "4. Enviar la solicitud a Ollama\n",
        "5. Formatear y devolver la respuesta\n",
        "\n",
        "### Ollama: Ejecución Local de Modelos de Lenguaje\n",
        "\n",
        "Ollama es una herramienta que facilita la ejecución de modelos de lenguaje grandes (LLMs) en tu propia máquina. Maneja automáticamente la descarga, configuración y ejecución de modelos.\n",
        "\n",
        "**Ventajas de LLMs locales:**\n",
        "- **Sin costos por uso:** No pagás por cada llamada a la API\n",
        "- **Privacidad:** Tus datos no salen de tu infraestructura\n",
        "- **Sin límites de rate:** No hay restricciones de requests por minuto\n",
        "- **Funciona offline:** No dependés de conexión a internet\n",
        "\n",
        "**Desventajas:**\n",
        "- Requiere recursos computacionales (RAM, CPU/GPU)\n",
        "- Los modelos ocupan espacio en disco\n",
        "- Generalmente son menos potentes que los modelos comerciales más grandes\n",
        "\n",
        "### IBM Granite 4: El Modelo de Lenguaje\n",
        "\n",
        "Granite 4 es la última generación de modelos de IBM, lanzada en octubre 2025. Tiene una arquitectura híbrida innovadora que combina capas Mamba-2 con capas Transformer tradicionales.\n",
        "\n",
        "**Características principales:**\n",
        "- **Eficiencia de memoria:** Reduce el uso de RAM en más del 70% comparado con modelos transformer puros\n",
        "- **Multilingüe:** Soporta español, inglés, francés, alemán, japonés, portugués, árabe, chino, entre otros\n",
        "- **Contexto largo:** Puede procesar hasta 512K tokens\n",
        "- **Optimizado para empresas:** Excelente en seguimiento de instrucciones y function calling\n",
        "\n",
        "**Tamaños disponibles:**\n",
        "- **granite4:micro** (3B parámetros) - Ideal para laptops, muy rápido\n",
        "- **granite4:tiny** (7B MoE, ~1B activos) - Balance entre velocidad y calidad\n",
        "- **granite4:latest** (32B MoE, ~9B activos) - Máxima calidad, requiere más recursos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_wywzb5SFKp"
      },
      "source": [
        "---\n",
        "\n",
        "## Prerrequisitos\n",
        "\n",
        "### Requisitos de Software\n",
        "\n",
        "#### 1. Docker Desktop\n",
        "\n",
        "**Para Windows o Mac:**\n",
        "1. Descargar desde: https://www.docker.com/products/docker-desktop\n",
        "2. Instalar siguiendo el asistente\n",
        "3. Reiniciar tu computadora\n",
        "4. Verificar que Docker Desktop está corriendo (icono de ballena en la barra de tareas)\n",
        "\n",
        "**Para Linux (Ubuntu/Debian):**\n",
        "```bash\n",
        "sudo apt update\n",
        "sudo apt install docker.io docker-compose\n",
        "sudo systemctl start docker\n",
        "sudo systemctl enable docker\n",
        "sudo usermod -aG docker $USER\n",
        "```\n",
        "\n",
        "Después de instalar, verificá que funciona:\n",
        "```bash\n",
        "docker --version\n",
        "docker-compose --version\n",
        "```\n",
        "\n",
        "Deberías ver algo como:\n",
        "```\n",
        "Docker version 24.0.7, build afdd53b\n",
        "Docker Compose version v2.23.0\n",
        "```\n",
        "\n",
        "#### 2. Editor de Texto\n",
        "\n",
        "Cualquier editor sirve: VSCode (recomendado), Sublime Text, Notepad++, Atom.\n",
        "\n",
        "#### 3. Terminal/Consola\n",
        "\n",
        "- **Windows:** PowerShell, CMD o Windows Terminal\n",
        "- **Mac:** Terminal (incluida en el sistema)\n",
        "- **Linux:** La terminal de tu distribución\n",
        "\n",
        "### Requisitos de Hardware\n",
        "\n",
        "**Mínimos (para granite4:micro):**\n",
        "- RAM: 8GB\n",
        "- Espacio en disco: 10GB libres\n",
        "- Procesador: Cualquier CPU moderna de los últimos 5 años\n",
        "\n",
        "**Recomendados (para granite4:tiny o latest):**\n",
        "- RAM: 16GB o más\n",
        "- Espacio en disco: 20GB libres\n",
        "- Procesador: CPU multinúcleo o GPU (opcional pero mejora la velocidad)\n",
        "\n",
        "### Conocimientos Previos Necesarios\n",
        "\n",
        "- Uso básico de la terminal/línea de comandos\n",
        "- Conceptos básicos de redes (qué es un puerto, localhost)\n",
        "- Conocimientos previos de la materia (modelos de lenguaje, tareas de NLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yePFAMA-SFKq"
      },
      "source": [
        "---\n",
        "\n",
        "## Paso 1: Preparar el Proyecto\n",
        "\n",
        "### Crear el Directorio del Proyecto\n",
        "\n",
        "Abrí tu terminal y ejecutá los siguientes comandos:\n",
        "\n",
        "**En Windows (PowerShell o CMD):**\n",
        "```powershell\n",
        "# Navegar a donde quieras crear el proyecto (ej: Documentos)\n",
        "cd C:\\Users\\TuUsuario\\Documents\n",
        "\n",
        "# Crear directorio\n",
        "mkdir asistente-nlp\n",
        "cd asistente-nlp\n",
        "```\n",
        "\n",
        "**En Mac o Linux:**\n",
        "```bash\n",
        "# Navegar a donde quieras crear el proyecto (ej: Documentos)\n",
        "cd ~/Documents\n",
        "\n",
        "# Crear directorio\n",
        "mkdir asistente-nlp\n",
        "cd asistente-nlp\n",
        "```\n",
        "\n",
        "### Crear el archivo docker-compose.yml\n",
        "\n",
        "Este archivo es el corazón de nuestro proyecto. Define qué servicios vamos a levantar y cómo van a interactuar entre sí.\n",
        "\n",
        "Abrí tu editor de texto y creá un archivo llamado `docker-compose.yml` en el directorio `asistente-nlp` con el siguiente contenido:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlHEOgASSFKq"
      },
      "outputs": [],
      "source": [
        "# IMPORTANTE: NO ejecutes esta celda.\n",
        "# Copia el contenido en un archivo llamado docker-compose.yml\n",
        "\n",
        "\"\"\"\n",
        "services:\n",
        "  n8n:\n",
        "    image: docker.n8n.io/n8nio/n8n\n",
        "    container_name: n8n\n",
        "    restart: unless-stopped\n",
        "    ports:\n",
        "      - \"5678:5678\"\n",
        "    environment:\n",
        "      - GENERIC_TIMEZONE=America/Argentina/Buenos_Aires\n",
        "      - TZ=America/Argentina/Buenos_Aires\n",
        "      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true\n",
        "      - N8N_RUNNERS_ENABLED=true\n",
        "    volumes:\n",
        "      - n8n_data:/home/node/.n8n\n",
        "    networks:\n",
        "      - n8n-network\n",
        "    depends_on:\n",
        "      - ollama\n",
        "\n",
        "  ollama:\n",
        "    image: ollama/ollama:latest\n",
        "    container_name: ollama\n",
        "    restart: unless-stopped\n",
        "    ports:\n",
        "      - \"11434:11434\"\n",
        "    environment:\n",
        "      - TZ=America/Argentina/Buenos_Aires\n",
        "    volumes:\n",
        "      - ollama_data:/root/.ollama\n",
        "    networks:\n",
        "      - n8n-network\n",
        "\n",
        "volumes:\n",
        "  n8n_data:\n",
        "  ollama_data:\n",
        "\n",
        "networks:\n",
        "  n8n-network:\n",
        "    driver: bridge\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VqVbeY_SFKq"
      },
      "source": [
        "### Entendiendo el docker-compose.yml\n",
        "\n",
        "Vamos a analizar cada sección:\n",
        "\n",
        "#### Servicio n8n\n",
        "\n",
        "```yaml\n",
        "n8n:\n",
        "  image: docker.n8n.io/n8nio/n8n\n",
        "```\n",
        "Esto le dice a Docker que descargue la imagen oficial de n8n desde su registro.\n",
        "\n",
        "```yaml\n",
        "  ports:\n",
        "    - \"5678:5678\"\n",
        "```\n",
        "Mapea el puerto 5678 del contenedor al puerto 5678 de tu máquina. Esto significa que vas a poder acceder a n8n en `http://localhost:5678`.\n",
        "\n",
        "```yaml\n",
        "  environment:\n",
        "    - GENERIC_TIMEZONE=America/Argentina/Buenos_Aires\n",
        "    - TZ=America/Argentina/Buenos_Aires\n",
        "```\n",
        "Configura la zona horaria. Podés cambiarla a la tuya si no estás en Argentina.\n",
        "\n",
        "```yaml\n",
        "  volumes:\n",
        "    - n8n_data:/home/node/.n8n\n",
        "```\n",
        "Esto es crucial. Un **volumen** es un espacio de almacenamiento persistente. Los contenedores Docker son efímeros: si los borrás, perdés todo lo que estaba adentro. Los volúmenes permiten guardar datos importantes (como tus workflows de n8n) que sobreviven aunque elimines el contenedor.\n",
        "\n",
        "```yaml\n",
        "  networks:\n",
        "    - n8n-network\n",
        "```\n",
        "Conecta este contenedor a una red privada llamada `n8n-network`. Esto permite que n8n y Ollama se comuniquen entre sí.\n",
        "\n",
        "```yaml\n",
        "  depends_on:\n",
        "    - ollama\n",
        "```\n",
        "Le indica a Docker que levante primero Ollama y después n8n.\n",
        "\n",
        "#### Servicio Ollama\n",
        "\n",
        "La configuración es similar. Cosas importantes:\n",
        "\n",
        "```yaml\n",
        "  ports:\n",
        "    - \"11434:11434\"\n",
        "```\n",
        "Ollama expone su API en el puerto 11434.\n",
        "\n",
        "```yaml\n",
        "  volumes:\n",
        "    - ollama_data:/root/.ollama\n",
        "```\n",
        "Acá se guardan los modelos descargados. Si descargás Granite 4 (que puede pesar varios GB), querés que se guarde en un volumen persistente para no tener que descargarlo de nuevo cada vez.\n",
        "\n",
        "#### Sección de Volúmenes y Redes\n",
        "\n",
        "```yaml\n",
        "volumes:\n",
        "  n8n_data:\n",
        "  ollama_data:\n",
        "```\n",
        "Declara los volúmenes que vamos a usar.\n",
        "\n",
        "```yaml\n",
        "networks:\n",
        "  n8n-network:\n",
        "    driver: bridge\n",
        "```\n",
        "Crea una red tipo \"bridge\" (puente) que conecta los contenedores. Dentro de esta red, los contenedores pueden referirse entre sí por su nombre (ej: n8n puede hacer requests a `http://ollama:11434`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEQZeWr9SFKq"
      },
      "source": [
        "---\n",
        "\n",
        "## Paso 2: Levantar los Servicios\n",
        "\n",
        "### Verificar Docker\n",
        "\n",
        "Antes de continuar, asegurate de que Docker Desktop esté corriendo. Deberías ver el ícono de la ballena en tu barra de tareas.\n",
        "\n",
        "En tu terminal, ejecutá:\n",
        "```bash\n",
        "docker ps\n",
        "```\n",
        "\n",
        "Si ves una tabla (aunque esté vacía), Docker está funcionando. Si ves un error, revisá que Docker Desktop esté iniciado.\n",
        "\n",
        "### Levantar los Contenedores\n",
        "\n",
        "Desde el directorio donde está tu `docker-compose.yml`, ejecutá:\n",
        "\n",
        "```bash\n",
        "docker-compose up -d\n",
        "```\n",
        "\n",
        "**¿Qué hace este comando?**\n",
        "- `docker-compose up`: Levanta los servicios definidos en docker-compose.yml\n",
        "- `-d`: \"Detached mode\" - corre los contenedores en segundo plano\n",
        "\n",
        "**¿Qué va a pasar?**\n",
        "\n",
        "La primera vez que ejecutes este comando:\n",
        "\n",
        "1. Docker va a descargar las imágenes de n8n y Ollama (esto puede tardar varios minutos según tu conexión)\n",
        "2. Va a crear los volúmenes `n8n_data` y `ollama_data`\n",
        "3. Va a crear la red `n8n-network`\n",
        "4. Va a iniciar ambos contenedores\n",
        "\n",
        "Vas a ver una salida similar a esta:\n",
        "```\n",
        "[+] Running 5/5\n",
        " ✔ Network asistente-nlp_n8n-network  Created\n",
        " ✔ Volume \"asistente-nlp_n8n_data\"    Created\n",
        " ✔ Volume \"asistente-nlp_ollama_data\" Created\n",
        " ✔ Container ollama                   Started\n",
        " ✔ Container n8n                      Started\n",
        "```\n",
        "\n",
        "### Verificar que los Servicios Están Corriendo\n",
        "\n",
        "```bash\n",
        "docker-compose ps\n",
        "```\n",
        "\n",
        "Deberías ver:\n",
        "```\n",
        "NAME      IMAGE                       STATUS         PORTS\n",
        "n8n       docker.n8n.io/n8nio/n8n    Up 2 minutes   0.0.0.0:5678->5678/tcp\n",
        "ollama    ollama/ollama:latest        Up 2 minutes   0.0.0.0:11434->11434/tcp\n",
        "```\n",
        "\n",
        "La columna STATUS debe decir \"Up\" para ambos servicios.\n",
        "\n",
        "### Ver los Logs (Útil para debugging)\n",
        "\n",
        "Si querés ver qué están haciendo los contenedores:\n",
        "\n",
        "```bash\n",
        "# Ver logs de ambos servicios\n",
        "docker-compose logs -f\n",
        "\n",
        "# Ver logs solo de n8n\n",
        "docker-compose logs -f n8n\n",
        "\n",
        "# Ver logs solo de Ollama\n",
        "docker-compose logs -f ollama\n",
        "```\n",
        "\n",
        "Presioná `Ctrl + C` para salir de los logs.\n",
        "\n",
        "### Comandos Útiles de Docker Compose\n",
        "\n",
        "Para referencia futura:\n",
        "\n",
        "```bash\n",
        "# Detener servicios (mantiene contenedores y datos)\n",
        "docker-compose stop\n",
        "\n",
        "# Iniciar servicios previamente detenidos\n",
        "docker-compose start\n",
        "\n",
        "# Reiniciar servicios\n",
        "docker-compose restart\n",
        "\n",
        "# Detener y eliminar contenedores (mantiene volúmenes)\n",
        "docker-compose down\n",
        "\n",
        "# CUIDADO: Eliminar TODO incluyendo volúmenes (perdés tus datos)\n",
        "docker-compose down -v\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIoy1m5wSFKr"
      },
      "source": [
        "---\n",
        "\n",
        "## Paso 3: Configurar Ollama y Descargar el Modelo\n",
        "\n",
        "### Verificar que Ollama está Funcionando\n",
        "\n",
        "Primero, verificá que Ollama esté respondiendo:\n",
        "\n",
        "```bash\n",
        "curl http://localhost:11434\n",
        "```\n",
        "\n",
        "Deberías ver: `Ollama is running`\n",
        "\n",
        "Si no tenés `curl` instalado en Windows, podés abrir `http://localhost:11434` en tu navegador y deberías ver el mismo mensaje.\n",
        "\n",
        "### Descargar el Modelo IBM Granite 4\n",
        "\n",
        "Ahora viene una decisión importante: elegir qué tamaño de modelo descargar. Esto depende de tu hardware.\n",
        "\n",
        "#### Opción A: granite4:micro (Recomendado para empezar)\n",
        "\n",
        "**Características:**\n",
        "- Tamaño: ~1.7GB\n",
        "- RAM necesaria: 4-6GB\n",
        "- Velocidad: Muy rápida\n",
        "- Ideal para: Laptops con recursos limitados, pruebas rápidas\n",
        "\n",
        "```bash\n",
        "docker exec -it ollama ollama pull granite4:micro\n",
        "```\n",
        "\n",
        "#### Opción B: granite4:tiny\n",
        "\n",
        "**Características:**\n",
        "- Tamaño: ~4GB\n",
        "- RAM necesaria: 8GB\n",
        "- Velocidad: Balanceada\n",
        "- Ideal para: PCs de escritorio con recursos moderados\n",
        "\n",
        "```bash\n",
        "docker exec -it ollama ollama pull granite4:tiny\n",
        "```\n",
        "\n",
        "#### Opción C: granite4:latest\n",
        "\n",
        "**Características:**\n",
        "- Tamaño: ~18GB\n",
        "- RAM necesaria: 16GB o más\n",
        "- Velocidad: Más lenta, pero mejor calidad\n",
        "- Ideal para: Workstations potentes, cuando necesitás la mejor calidad\n",
        "\n",
        "```bash\n",
        "docker exec -it ollama ollama pull granite4:latest\n",
        "```\n",
        "\n",
        "### Entendiendo el Comando\n",
        "\n",
        "```bash\n",
        "docker exec -it ollama ollama pull granite4:micro\n",
        "```\n",
        "\n",
        "- `docker exec`: Ejecuta un comando dentro de un contenedor que ya está corriendo\n",
        "- `-it`: Modo interactivo con terminal (para ver el progreso de la descarga)\n",
        "- `ollama`: Nombre del contenedor donde queremos ejecutar el comando\n",
        "- `ollama pull granite4:micro`: El comando específico de Ollama para descargar un modelo\n",
        "\n",
        "La descarga puede tardar varios minutos. Vas a ver una barra de progreso.\n",
        "\n",
        "### Verificar que el Modelo se Descargó\n",
        "\n",
        "```bash\n",
        "docker exec -it ollama ollama list\n",
        "```\n",
        "\n",
        "Deberías ver algo como:\n",
        "```\n",
        "NAME                  ID              SIZE      MODIFIED\n",
        "granite4:micro        abc123def456    1.7 GB    2 minutes ago\n",
        "```\n",
        "\n",
        "### Probar el Modelo Interactivamente\n",
        "\n",
        "Antes de integrarlo con n8n, es buena idea probarlo para verificar que funciona:\n",
        "\n",
        "```bash\n",
        "docker exec -it ollama ollama run granite4:micro\n",
        "```\n",
        "\n",
        "Esto abre una sesión interactiva. Podés escribir consultas y el modelo va a responder. Por ejemplo, probá:\n",
        "\n",
        "```\n",
        ">>> Hola, ¿cómo estás?\n",
        "```\n",
        "\n",
        "o\n",
        "\n",
        "```\n",
        ">>> Extrae las entidades nombradas de: María García trabaja en Microsoft Argentina\n",
        "```\n",
        "\n",
        "Para salir de la sesión interactiva, escribí:\n",
        "```\n",
        "/bye\n",
        "```\n",
        "\n",
        "### Comandos Útiles de Ollama\n",
        "\n",
        "```bash\n",
        "# Listar modelos instalados\n",
        "docker exec -it ollama ollama list\n",
        "\n",
        "# Ver información detallada de un modelo\n",
        "docker exec -it ollama ollama show granite4:micro\n",
        "\n",
        "# Eliminar un modelo (para liberar espacio)\n",
        "docker exec -it ollama ollama rm granite4:micro\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1fLUTSISFKr"
      },
      "source": [
        "---\n",
        "\n",
        "## Paso 4: Configuración Inicial de n8n\n",
        "\n",
        "### Acceder a n8n\n",
        "\n",
        "Abrí tu navegador web y andá a:\n",
        "\n",
        "```\n",
        "http://localhost:5678\n",
        "```\n",
        "\n",
        "### Primera Configuración\n",
        "\n",
        "Si es la primera vez que accedés a n8n, vas a ver una pantalla de bienvenida.\n",
        "\n",
        "1. **Email:** Ingresá un email (puede ser cualquiera, es solo para tu sesión local)\n",
        "2. **First name y Last name:** Tu nombre\n",
        "3. **Password:** Elegí una contraseña segura\n",
        "4. Click en **\"Get Started\"**\n",
        "\n",
        "**Nota:** Esta configuración se guarda en el volumen Docker `n8n_data`. Si eliminás ese volumen, vas a perder tu usuario y tus workflows.\n",
        "\n",
        "### Interfaz de n8n\n",
        "\n",
        "Vas a ver el dashboard de n8n con:\n",
        "- **Workflows:** Lista de tus workflows (vacía por ahora)\n",
        "- **Credentials:** Donde guardás claves API y configuraciones\n",
        "- **Executions:** Historial de ejecuciones de workflows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfphN4R_SFKr"
      },
      "source": [
        "---\n",
        "\n",
        "## Paso 5: Construir el Workflow de NLP\n",
        "\n",
        "Ahora viene la parte central del proyecto: construir el workflow que va a procesar las solicitudes de NLP. En lugar de importar un archivo JSON, vamos a construirlo paso a paso para que entiendas cómo funciona cada componente.\n",
        "\n",
        "### Crear un Nuevo Workflow\n",
        "\n",
        "1. En n8n, click en el botón **\"+ Add workflow\"** (esquina superior derecha)\n",
        "2. Dale un nombre al workflow: **\"Asistente NLP\"**\n",
        "3. Vas a ver un canvas en blanco donde vamos a armar el flujo\n",
        "\n",
        "### Arquitectura del Workflow\n",
        "\n",
        "Nuestro workflow va a tener 5 nodos conectados en secuencia:\n",
        "\n",
        "```\n",
        "1. Webhook           →    Recibe solicitudes HTTP POST\n",
        "2. Extraer Mensaje   →    Extrae el campo \"mensaje\" del JSON\n",
        "3. Detectar Tarea    →    Identifica qué tarea de NLP hacer\n",
        "4. Ollama            →    Envía el prompt al modelo\n",
        "5. Formatear         →    Prepara la respuesta final\n",
        "```\n",
        "\n",
        "### Nodo 1: Webhook (Punto de Entrada)\n",
        "\n",
        "El webhook es el punto de entrada de nuestro sistema. Va a recibir solicitudes HTTP POST con las consultas del usuario.\n",
        "\n",
        "**Pasos:**\n",
        "\n",
        "1. Click en **\"+ Add first step\"** (en el canvas vacío)\n",
        "2. En el buscador, escribí **\"Webhook\"**\n",
        "3. Seleccioná **\"Webhook\"** de la categoría \"Core Nodes\"\n",
        "4. Configurá el nodo:\n",
        "   - **HTTP Method:** POST\n",
        "   - **Path:** asistente-nlp\n",
        "   - **Response Mode:** Last Node\n",
        "   - Dejá el resto como está\n",
        "\n",
        "**¿Qué hace este nodo?**\n",
        "\n",
        "Crea un endpoint HTTP en: `http://localhost:5678/webhook/asistente-nlp`\n",
        "\n",
        "Cuando alguien envía un POST a esa URL con un JSON como:\n",
        "```json\n",
        "{\"mensaje\": \"Extrae entidades de: María vive en Buenos Aires\"}\n",
        "```\n",
        "\n",
        "El webhook lo recibe y pasa los datos al siguiente nodo.\n",
        "\n",
        "### Nodo 2: Extraer Mensaje\n",
        "\n",
        "Este nodo va a extraer el campo \"mensaje\" del JSON recibido y también va a agregar un timestamp.\n",
        "\n",
        "**Pasos:**\n",
        "\n",
        "1. Hover sobre el webhook y click en el **+** que aparece a la derecha\n",
        "2. Buscá y seleccioná **\"Edit Fields (Set)\"**\n",
        "3. Cambiá el nombre del nodo a: **\"Extraer Mensaje\"**\n",
        "4. En **\"Fields to Set\"**, agregá dos campos:\n",
        "\n",
        "   **Campo 1:**\n",
        "   - **Name:** mensaje_usuario\n",
        "   - **Type:** String\n",
        "   - **Value:** `{{ $json.body.mensaje || '' }}`\n",
        "   \n",
        "   **Campo 2:**\n",
        "   - **Name:** timestamp\n",
        "   - **Type:** String  \n",
        "   - **Value:** `{{ $now.toISO() }}`\n",
        "\n",
        "**¿Qué hace este nodo?**\n",
        "\n",
        "- Extrae el campo `mensaje` del body del request\n",
        "- Crea un timestamp con la hora actual\n",
        "- Pasa estos datos al siguiente nodo\n",
        "\n",
        "La sintaxis `{{ }}` es la forma de acceder a datos en n8n. Es similar a templates en otros frameworks.\n",
        "\n",
        "### Nodo 3: Detectar Tarea NLP\n",
        "\n",
        "Este es el cerebro de nuestro sistema. Va a analizar el mensaje del usuario y decidir qué tipo de tarea de NLP quiere realizar, construyendo un prompt especializado.\n",
        "\n",
        "**Pasos:**\n",
        "\n",
        "1. Click en el **+** después de \"Extraer Mensaje\"\n",
        "2. Buscá y seleccioná **\"Code\"**\n",
        "3. Cambiá el nombre a: **\"Detectar Tarea NLP\"**\n",
        "4. Seleccioná **\"Run Once for All Items\"** en Mode\n",
        "5. En el editor de código JavaScript, pegá el siguiente código:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yjaSSEzSFKr"
      },
      "outputs": [],
      "source": [
        "# Código JavaScript para el nodo \"Detectar Tarea NLP\"\n",
        "# Copiar y pegar en el editor de n8n\n",
        "\n",
        "\"\"\"\n",
        "const inputData = $input.first().json;\n",
        "const mensaje = (inputData.mensaje_usuario || '').toString().toLowerCase();\n",
        "const timestamp = inputData.timestamp;\n",
        "\n",
        "let prompt_final = \"\";\n",
        "let tarea_detectada = \"general\";\n",
        "let instrucciones = \"\";\n",
        "\n",
        "// Detección de tarea NER\n",
        "if (mensaje.includes(\"ner\") || mensaje.includes(\"entidades\") || mensaje.includes(\"nombres\")) {\n",
        "  tarea_detectada = \"NER\";\n",
        "  instrucciones = \"Extrae todas las entidades nombradas y clasifícalas.\";\n",
        "  prompt_final = `Eres un experto en Named Entity Recognition (NER).\n",
        "\n",
        "Tarea: Extrae TODAS las entidades nombradas del texto y clasifícalas en:\n",
        "- PERSONA: nombres de personas\n",
        "- ORGANIZACIÓN: empresas, instituciones\n",
        "- LUGAR: ciudades, países, lugares\n",
        "- FECHA: fechas, períodos temporales\n",
        "- CANTIDAD: números, porcentajes, montos\n",
        "\n",
        "Formato de respuesta:\n",
        "Entidad | Tipo | Contexto\n",
        "\n",
        "Texto a analizar:\n",
        "${mensaje}`;\n",
        "\n",
        "// Detección de análisis de sentimiento\n",
        "} else if (mensaje.includes(\"sentimiento\") || mensaje.includes(\"sentiment\") || mensaje.includes(\"emoción\")) {\n",
        "  tarea_detectada = \"Análisis de Sentimiento\";\n",
        "  instrucciones = \"Analiza el sentimiento del texto.\";\n",
        "  prompt_final = `Eres un experto en Análisis de Sentimiento.\n",
        "\n",
        "Tarea: Analiza el sentimiento del siguiente texto.\n",
        "\n",
        "Clasifícalo como:\n",
        "- POSITIVO\n",
        "- NEGATIVO\n",
        "- NEUTRAL\n",
        "\n",
        "Proporciona:\n",
        "1. Clasificación del sentimiento\n",
        "2. Nivel de confianza (0-100%)\n",
        "3. Palabras clave que indican el sentimiento\n",
        "4. Explicación breve\n",
        "\n",
        "Texto a analizar:\n",
        "${mensaje}`;\n",
        "\n",
        "// Detección de sumarización\n",
        "} else if (mensaje.includes(\"resume\") || mensaje.includes(\"resumen\") || mensaje.includes(\"sumari\")) {\n",
        "  tarea_detectada = \"Sumarización\";\n",
        "  instrucciones = \"Resume el texto en puntos clave.\";\n",
        "  prompt_final = `Eres un experto en Sumarización de textos.\n",
        "\n",
        "Tarea: Resume el siguiente texto de forma concisa.\n",
        "\n",
        "Proporciona:\n",
        "1. Resumen en 3-5 puntos clave\n",
        "2. Idea principal\n",
        "3. Longitud aproximada: 20% del texto original\n",
        "\n",
        "Texto a analizar:\n",
        "${mensaje}`;\n",
        "\n",
        "// Detección de clasificación\n",
        "} else if (mensaje.includes(\"clasifica\") || mensaje.includes(\"categoría\") || mensaje.includes(\"tipo de texto\")) {\n",
        "  tarea_detectada = \"Clasificación\";\n",
        "  instrucciones = \"Clasifica el tipo y tema del texto.\";\n",
        "  prompt_final = `Eres un experto en Clasificación de Textos.\n",
        "\n",
        "Tarea: Clasifica el siguiente texto.\n",
        "\n",
        "Proporciona:\n",
        "1. Tipo de texto (noticia, opinión, técnico, narrativo, etc.)\n",
        "2. Tema principal\n",
        "3. Temas secundarios\n",
        "4. Nivel de formalidad (formal/informal)\n",
        "5. Audiencia objetivo\n",
        "\n",
        "Texto a analizar:\n",
        "${mensaje}`;\n",
        "\n",
        "// Detección de extracción de keywords\n",
        "} else if (mensaje.includes(\"palabras clave\") || mensaje.includes(\"keywords\") || mensaje.includes(\"términos importantes\")) {\n",
        "  tarea_detectada = \"Extracción de Keywords\";\n",
        "  instrucciones = \"Extrae las palabras clave más importantes.\";\n",
        "  prompt_final = `Eres un experto en Extracción de Palabras Clave.\n",
        "\n",
        "Tarea: Extrae las palabras clave más relevantes del texto.\n",
        "\n",
        "Proporciona:\n",
        "1. Top 10 palabras clave\n",
        "2. Relevancia de cada una (Alta/Media/Baja)\n",
        "3. Bigramas importantes (frases de 2 palabras)\n",
        "4. Tema central\n",
        "\n",
        "Texto a analizar:\n",
        "${mensaje}`;\n",
        "\n",
        "// Ayuda\n",
        "} else if (mensaje.includes(\"ayuda\") || mensaje.includes(\"help\") || mensaje.includes(\"qué puedes hacer\")) {\n",
        "  tarea_detectada = \"Ayuda\";\n",
        "  prompt_final = `ASISTENTE NLP - TAREAS DISPONIBLES\n",
        "\n",
        "Puedo ayudarte con las siguientes tareas de Procesamiento de Lenguaje Natural:\n",
        "\n",
        "1. NER (Named Entity Recognition)\n",
        "   Comando: \"Extrae entidades de: [tu texto]\"\n",
        "   Ejemplo: \"Extrae entidades de: Juan viajó a Madrid en 2024\"\n",
        "\n",
        "2. Análisis de Sentimiento\n",
        "   Comando: \"Analiza el sentimiento de: [tu texto]\"\n",
        "   Ejemplo: \"Analiza el sentimiento de: Me encanta este producto\"\n",
        "\n",
        "3. Sumarización\n",
        "   Comando: \"Resume: [tu texto]\"\n",
        "   Ejemplo: \"Resume: [texto largo]\"\n",
        "\n",
        "4. Clasificación de Texto\n",
        "   Comando: \"Clasifica: [tu texto]\"\n",
        "   Ejemplo: \"Clasifica: Este artículo habla sobre inteligencia artificial\"\n",
        "\n",
        "5. Extracción de Palabras Clave\n",
        "   Comando: \"Palabras clave de: [tu texto]\"\n",
        "   Ejemplo: \"Palabras clave de: [tu texto]\"\n",
        "\n",
        "Tip: Simplemente escribe tu solicitud de forma natural`;\n",
        "\n",
        "// Caso general\n",
        "} else {\n",
        "  tarea_detectada = \"Consulta General\";\n",
        "  instrucciones = \"Responde la consulta del usuario.\";\n",
        "  prompt_final = mensaje;\n",
        "}\n",
        "\n",
        "// Retornar datos estructurados\n",
        "return [{\n",
        "  json: {\n",
        "    prompt: prompt_final,\n",
        "    tarea: tarea_detectada,\n",
        "    instrucciones: instrucciones,\n",
        "    mensaje_original: mensaje,\n",
        "    timestamp: timestamp\n",
        "  }\n",
        "}];\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHxyGpvkSFKr"
      },
      "source": [
        "**¿Qué hace este código?**\n",
        "\n",
        "1. Lee el mensaje del usuario\n",
        "2. Lo convierte a minúsculas para facilitar la detección\n",
        "3. Busca palabras clave (\"ner\", \"sentimiento\", \"resume\", etc.)\n",
        "4. Según las palabras encontradas, construye un prompt especializado para esa tarea\n",
        "5. Retorna un objeto con el prompt, el tipo de tarea detectada y metadata\n",
        "\n",
        "Por ejemplo, si el usuario escribe:\n",
        "```\n",
        "Extrae entidades de: María trabaja en Google\n",
        "```\n",
        "\n",
        "El código detecta \"entidades\" y construye un prompt especializado que le pide al modelo que actúe como experto en NER.\n",
        "\n",
        "### Nodo 4: Ollama (Conexión con el Modelo)\n",
        "\n",
        "Este nodo envía el prompt al modelo de lenguaje y recibe la respuesta.\n",
        "\n",
        "**Pasos:**\n",
        "\n",
        "1. Click en el **+** después de \"Detectar Tarea NLP\"\n",
        "2. Buscá y seleccioná **\"HTTP Request\"**\n",
        "3. Cambiá el nombre a: **\"Ollama\"**\n",
        "4. Configurá:\n",
        "   - **Method:** POST\n",
        "   - **URL:** `http://ollama:11434/api/generate`\n",
        "   - **Authentication:** None\n",
        "   - **Send Body:** Activado\n",
        "   - **Body Content Type:** JSON\n",
        "   - **Specify Body:** Using JSON\n",
        "5. En el campo **JSON**, pegá:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBX-4MoLSFKr"
      },
      "outputs": [],
      "source": [
        "# JSON para el nodo Ollama\n",
        "# Copiar en el campo JSON del HTTP Request\n",
        "\n",
        "\"\"\"\n",
        "{\n",
        "  \"model\": \"granite4:micro\",\n",
        "  \"prompt\": \"={{ $json.prompt }}\",\n",
        "  \"stream\": false,\n",
        "  \"temperature\": 0.7\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZVv6vw4SFKs"
      },
      "source": [
        "**Importante:** Si descargaste otro tamaño de modelo (tiny o latest), cambiá `\"granite4:micro\"` por el que descargaste.\n",
        "\n",
        "**¿Qué hace este nodo?**\n",
        "\n",
        "Envía un request POST a la API de Ollama con:\n",
        "- **model:** El modelo a usar\n",
        "- **prompt:** El prompt construido por el nodo anterior (accede con `$json.prompt`)\n",
        "- **stream:** false (queremos la respuesta completa, no por streaming)\n",
        "- **temperature:** 0.7 (controla la creatividad del modelo, 0=determinista, 1=creativo)\n",
        "\n",
        "**¿Por qué `http://ollama:11434` y no `http://localhost:11434`?**\n",
        "\n",
        "Dentro de la red Docker, los contenedores se refieren entre sí por su nombre. n8n está en un contenedor y Ollama en otro. Ambos están en la red `n8n-network`, entonces n8n puede acceder a Ollama usando `http://ollama:11434`.\n",
        "\n",
        "### Nodo 5: Formatear Respuesta\n",
        "\n",
        "Este nodo final formatea la respuesta para devolver al usuario.\n",
        "\n",
        "**Pasos:**\n",
        "\n",
        "1. Click en el **+** después de \"Ollama\"\n",
        "2. Seleccioná **\"Edit Fields (Set)\"**\n",
        "3. Cambiá el nombre a: **\"Formatear Respuesta\"**\n",
        "4. Agregá los siguientes campos:\n",
        "\n",
        "   **Campo 1:**\n",
        "   - **Name:** respuesta\n",
        "   - **Type:** String\n",
        "   - **Value:** `{{ $json.response }}`\n",
        "   \n",
        "   **Campo 2:**\n",
        "   - **Name:** tarea_realizada\n",
        "   - **Type:** String\n",
        "   - **Value:** `{{ $('Detectar Tarea NLP').item.json.tarea }}`\n",
        "   \n",
        "   **Campo 3:**\n",
        "   - **Name:** mensaje_original\n",
        "   - **Type:** String\n",
        "   - **Value:** `{{ $('Detectar Tarea NLP').item.json.mensaje_original }}`\n",
        "   \n",
        "   **Campo 4:**\n",
        "   - **Name:** timestamp\n",
        "   - **Type:** String\n",
        "   - **Value:** `{{ $now.toISO() }}`\n",
        "\n",
        "**¿Qué hace este nodo?**\n",
        "\n",
        "Crea un objeto JSON estructurado con:\n",
        "- La respuesta del modelo\n",
        "- Qué tipo de tarea se realizó\n",
        "- El mensaje original del usuario\n",
        "- Un timestamp de cuándo se completó\n",
        "\n",
        "Este es el formato que se devuelve al usuario.\n",
        "\n",
        "### Guardar y Activar el Workflow\n",
        "\n",
        "1. Click en **\"Save\"** (esquina superior derecha)\n",
        "2. **MUY IMPORTANTE:** Activá el workflow con el toggle **\"Active\"** (debe ponerse en verde/azul)\n",
        "\n",
        "Si no activás el workflow, no va a responder a las solicitudes.\n",
        "\n",
        "### Probar el Workflow desde n8n\n",
        "\n",
        "Antes de probar desde fuera, podés probar directamente en n8n:\n",
        "\n",
        "1. Click en el nodo **\"Webhook\"**\n",
        "2. Click en **\"Listen for Test Event\"**\n",
        "3. En otra pestaña del navegador, o desde tu terminal, enviá un request:\n",
        "\n",
        "```bash\n",
        "curl -X POST http://localhost:5678/webhook/asistente-nlp \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"mensaje\": \"Extrae entidades de: María trabaja en Google en Buenos Aires\"}'\n",
        "```\n",
        "\n",
        "4. Vas a ver que n8n recibe el request y podés hacer click en **\"Execute Workflow\"** para ejecutar todo el flujo\n",
        "5. Observá cómo los datos van pasando por cada nodo\n",
        "\n",
        "Esto te permite debuggear y ver exactamente qué está pasando en cada paso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OjXHpvfSFKs"
      },
      "source": [
        "---\n",
        "\n",
        "## Paso 6: Crear la Interfaz de Usuario\n",
        "\n",
        "Ahora que tenés el workflow funcionando, necesitás una forma para que los usuarios interactúen con él. Vamos a crear una interfaz web simple.\n",
        "\n",
        "### Opción A: Interfaz HTML Básica\n",
        "\n",
        "Creá un archivo `index.html` en tu directorio del proyecto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1yC6WIGSFKs"
      },
      "outputs": [],
      "source": [
        "# Contenido del archivo index.html\n",
        "# Guardar en tu directorio del proyecto\n",
        "\n",
        "\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Asistente NLP</title>\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: Arial, sans-serif;\n",
        "            max-width: 800px;\n",
        "            margin: 50px auto;\n",
        "            padding: 20px;\n",
        "            background-color: #f5f5f5;\n",
        "        }\n",
        "        h1 {\n",
        "            color: #333;\n",
        "            text-align: center;\n",
        "        }\n",
        "        .info-box {\n",
        "            background-color: #fff;\n",
        "            border: 1px solid #ddd;\n",
        "            border-radius: 5px;\n",
        "            padding: 20px;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        textarea {\n",
        "            width: 100%;\n",
        "            padding: 10px;\n",
        "            border: 1px solid #ddd;\n",
        "            border-radius: 5px;\n",
        "            font-size: 14px;\n",
        "            font-family: Arial, sans-serif;\n",
        "            resize: vertical;\n",
        "        }\n",
        "        button {\n",
        "            background-color: #007bff;\n",
        "            color: white;\n",
        "            padding: 12px 30px;\n",
        "            border: none;\n",
        "            border-radius: 5px;\n",
        "            font-size: 16px;\n",
        "            cursor: pointer;\n",
        "            display: block;\n",
        "            margin: 20px auto;\n",
        "        }\n",
        "        button:hover {\n",
        "            background-color: #0056b3;\n",
        "        }\n",
        "        #respuesta {\n",
        "            background-color: #f9f9f9;\n",
        "        }\n",
        "        .example {\n",
        "            color: #666;\n",
        "            font-size: 13px;\n",
        "            margin: 5px 0;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Asistente de Procesamiento de Lenguaje Natural</h1>\n",
        "\n",
        "    <div class=\"info-box\">\n",
        "        <h3>¿Cómo usar el asistente?</h3>\n",
        "        <p>Iniciá tu mensaje con alguna de estas palabras clave:</p>\n",
        "        <ul>\n",
        "            <li><strong>\"ner\"</strong> o <strong>\"entidades\"</strong> → Extracción de entidades nombradas</li>\n",
        "            <li><strong>\"sentimiento\"</strong> → Análisis de sentimiento</li>\n",
        "            <li><strong>\"resumen\"</strong> o <strong>\"resume\"</strong> → Sumarización</li>\n",
        "            <li><strong>\"clasifica\"</strong> → Clasificación de texto</li>\n",
        "            <li><strong>\"palabras clave\"</strong> → Extracción de keywords</li>\n",
        "            <li><strong>\"ayuda\"</strong> → Ver información completa</li>\n",
        "        </ul>\n",
        "        <p class=\"example\"><strong>Ejemplo:</strong> Extrae entidades de: María García trabaja en Microsoft Argentina</p>\n",
        "    </div>\n",
        "\n",
        "    <div>\n",
        "        <h3>Tu consulta:</h3>\n",
        "        <textarea id=\"mensaje\" rows=\"6\" placeholder=\"Escribí acá tu consulta o texto para analizar...\"></textarea>\n",
        "    </div>\n",
        "\n",
        "    <button onclick=\"enviar()\">Enviar</button>\n",
        "\n",
        "    <div>\n",
        "        <h3>Respuesta del asistente:</h3>\n",
        "        <textarea id=\"respuesta\" rows=\"15\" readonly></textarea>\n",
        "    </div>\n",
        "\n",
        "    <p style=\"text-align: center; color: #666; margin-top: 30px;\">\n",
        "        Asistente NLP - Procesamiento de Lenguaje Natural con n8n + Ollama\n",
        "    </p>\n",
        "\n",
        "    <script>\n",
        "        async function enviar() {\n",
        "            const mensaje = document.getElementById('mensaje').value;\n",
        "            const respuestaTextarea = document.getElementById('respuesta');\n",
        "\n",
        "            if (!mensaje.trim()) {\n",
        "                respuestaTextarea.value = 'Por favor, escribí tu consulta antes de enviar.';\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            respuestaTextarea.value = 'Procesando tu consulta...\\n\\nEsto puede tomar unos segundos.';\n",
        "\n",
        "            try {\n",
        "                const response = await fetch('http://localhost:5678/webhook/asistente-nlp', {\n",
        "                    method: 'POST',\n",
        "                    headers: { 'Content-Type': 'application/json' },\n",
        "                    body: JSON.stringify({ mensaje: mensaje })\n",
        "                });\n",
        "\n",
        "                if (!response.ok) {\n",
        "                    throw new Error(`HTTP ${response.status}: ${await response.text()}`);\n",
        "                }\n",
        "\n",
        "                const data = await response.json();\n",
        "\n",
        "                let output = '';\n",
        "                if (data.respuesta) {\n",
        "                    output += data.respuesta;\n",
        "                } else {\n",
        "                    output = JSON.stringify(data, null, 2);\n",
        "                }\n",
        "\n",
        "                if (data.tarea_realizada) {\n",
        "                    output += '\\n\\n─────────────────────────────\\n';\n",
        "                    output += 'Tarea: ' + data.tarea_realizada + '\\n';\n",
        "                    output += 'Timestamp: ' + data.timestamp;\n",
        "                }\n",
        "\n",
        "                respuestaTextarea.value = output;\n",
        "            } catch (error) {\n",
        "                respuestaTextarea.value = 'Error al procesar la solicitud:\\n\\n' + error.message +\n",
        "                    '\\n\\nVerificá que el workflow esté activo en n8n y que Ollama esté ejecutándose.';\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Permitir enviar con Ctrl+Enter\n",
        "        document.getElementById('mensaje').addEventListener('keydown', function(e) {\n",
        "            if (e.ctrlKey && e.key === 'Enter') {\n",
        "                enviar();\n",
        "            }\n",
        "        });\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ske2xp9NSFKs"
      },
      "source": [
        "Para usar esta interfaz:\n",
        "\n",
        "1. Abrí el archivo `index.html` con tu navegador (doble click o arrastrar al navegador)\n",
        "2. Escribí tu consulta\n",
        "3. Click en \"Enviar\"\n",
        "\n",
        "### Opción B: Interfaz con Streamlit (Más Avanzada)\n",
        "\n",
        "Ya conocés Streamlit de clases anteriores. Podés crear una interfaz más profesional.\n",
        "\n",
        "Creá un archivo `app_streamlit.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KZmJTkFSFKs"
      },
      "outputs": [],
      "source": [
        "# app_streamlit.py\n",
        "import streamlit as st\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Asistente NLP\",\n",
        "    page_icon=\"🤖\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\"Asistente de Procesamiento de Lenguaje Natural\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# Sidebar con información\n",
        "with st.sidebar:\n",
        "    st.header(\"Tareas Disponibles\")\n",
        "    st.markdown(\"\"\"\n",
        "    - **NER**: Extracción de entidades\n",
        "    - **Sentimiento**: Análisis emocional\n",
        "    - **Resumen**: Sumarización de textos\n",
        "    - **Clasificación**: Tipo y tema\n",
        "    - **Keywords**: Palabras clave\n",
        "    \"\"\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"**Ejemplos:**\")\n",
        "    st.code('Extrae entidades de: Juan vive en Madrid')\n",
        "    st.code('Analiza el sentimiento de: Me encantó')\n",
        "\n",
        "# Área principal\n",
        "mensaje = st.text_area(\n",
        "    \"Tu consulta:\",\n",
        "    height=150,\n",
        "    placeholder=\"Escribí acá tu consulta o texto para analizar...\"\n",
        ")\n",
        "\n",
        "if st.button(\"Analizar\", use_container_width=True, type=\"primary\"):\n",
        "    if mensaje.strip():\n",
        "        with st.spinner('Procesando tu consulta...'):\n",
        "            try:\n",
        "                response = requests.post(\n",
        "                    \"http://localhost:5678/webhook/asistente-nlp\",\n",
        "                    json={\"mensaje\": mensaje},\n",
        "                    timeout=60\n",
        "                )\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    st.success(\"Análisis completado\")\n",
        "\n",
        "                    st.markdown(\"### Resultado:\")\n",
        "                    st.markdown(data.get('respuesta', 'Sin respuesta'))\n",
        "\n",
        "                    st.markdown(\"---\")\n",
        "                    col1, col2 = st.columns(2)\n",
        "                    with col1:\n",
        "                        st.metric(\"Tarea\", data.get('tarea_realizada', 'N/A'))\n",
        "                    with col2:\n",
        "                        timestamp = data.get('timestamp', '')\n",
        "                        if timestamp:\n",
        "                            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))\n",
        "                            st.metric(\"Hora\", dt.strftime('%H:%M:%S'))\n",
        "                else:\n",
        "                    st.error(f\"Error HTTP {response.status_code}\")\n",
        "                    st.code(response.text)\n",
        "\n",
        "            except requests.exceptions.ConnectionError:\n",
        "                st.error(\"No se pudo conectar con n8n. Verificá que esté corriendo.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error: {str(e)}\")\n",
        "    else:\n",
        "        st.warning(\"Por favor, escribí tu consulta antes de enviar.\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"Asistente NLP - n8n + Ollama + Granite 4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXJpYCAuSFKs"
      },
      "source": [
        "Para ejecutar la app Streamlit:\n",
        "\n",
        "```bash\n",
        "# Instalar Streamlit (si no lo tenés)\n",
        "pip install streamlit requests\n",
        "\n",
        "# Ejecutar\n",
        "streamlit run app_streamlit.py\n",
        "```\n",
        "\n",
        "La app se va a abrir en `http://localhost:8501`\n",
        "\n",
        "### Pruebas Sugeridas\n",
        "\n",
        "Probá tu asistente con estas consultas:\n",
        "\n",
        "**1. NER:**\n",
        "```\n",
        "Extrae entidades de: María García trabaja en Microsoft Argentina en Buenos Aires desde marzo de 2023\n",
        "```\n",
        "\n",
        "**2. Sentimiento:**\n",
        "```\n",
        "Analiza el sentimiento de: El curso me pareció excelente, aprendí muchísimo y los profesores fueron muy claros en sus explicaciones\n",
        "```\n",
        "\n",
        "**3. Resumen:**\n",
        "```\n",
        "Resume: Docker es una plataforma de containerización que permite empaquetar aplicaciones con todas sus dependencias. Los contenedores son más livianos que las máquinas virtuales porque comparten el kernel del sistema operativo. Docker Compose facilita la orquestación de múltiples contenedores, permitiendo definir toda la infraestructura en un archivo YAML.\n",
        "```\n",
        "\n",
        "**4. Clasificación:**\n",
        "```\n",
        "Clasifica: La inteligencia artificial está revolucionando el procesamiento del lenguaje natural mediante el uso de modelos de lenguaje grandes\n",
        "```\n",
        "\n",
        "**5. Keywords:**\n",
        "```\n",
        "Palabras clave de: El procesamiento del lenguaje natural es una rama de la inteligencia artificial que se enfoca en la interacción entre computadoras y humanos usando lenguaje natural\n",
        "```\n",
        "\n",
        "**6. Ayuda:**\n",
        "```\n",
        "ayuda\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsIqoHEDSFKs"
      },
      "source": [
        "---\n",
        "\n",
        "## Paso 7: Crear una Imagen Docker Personalizada\n",
        "\n",
        "Hasta ahora usamos imágenes pre-construidas (n8n y Ollama). Ahora vas a aprender a crear tu propia imagen Docker que contenga tu aplicación Streamlit.\n",
        "\n",
        "### ¿Por qué crear una imagen?\n",
        "\n",
        "- **Portabilidad:** Podés compartir tu aplicación completa, no solo el código\n",
        "- **Reproducibilidad:** La imagen incluye todas las dependencias exactas\n",
        "- **Despliegue:** Facilita el deployment en servidores\n",
        "- **Versionado:** Podés tener múltiples versiones (v1.0, v1.1, v2.0)\n",
        "\n",
        "### Crear el Dockerfile\n",
        "\n",
        "Un Dockerfile es un archivo de texto con instrucciones para construir una imagen.\n",
        "\n",
        "Creá un archivo llamado `Dockerfile` (sin extensión) en tu proyecto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj7mEW8lSFKs"
      },
      "outputs": [],
      "source": [
        "# Contenido del Dockerfile\n",
        "\n",
        "\"\"\"\n",
        "# Imagen base: Python 3.11 slim (versión liviana)\n",
        "FROM python:3.11-slim\n",
        "\n",
        "# Metadata de la imagen\n",
        "LABEL maintainer=\"tu_email@example.com\"\n",
        "LABEL description=\"Asistente NLP con Streamlit\"\n",
        "LABEL version=\"1.0\"\n",
        "\n",
        "# Directorio de trabajo dentro del contenedor\n",
        "WORKDIR /app\n",
        "\n",
        "# Copiar archivo de dependencias\n",
        "COPY requirements.txt .\n",
        "\n",
        "# Instalar dependencias Python\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copiar archivos de la aplicación\n",
        "COPY app_streamlit.py .\n",
        "\n",
        "# Exponer puerto de Streamlit\n",
        "EXPOSE 8501\n",
        "\n",
        "# Comando para ejecutar la aplicación\n",
        "CMD [\"streamlit\", \"run\", \"app_streamlit.py\", \"--server.address\", \"0.0.0.0\"]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKjuwjqwSFKs"
      },
      "source": [
        "### Crear el archivo requirements.txt\n",
        "\n",
        "Lista las dependencias Python de tu aplicación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhs7MvyCSFKs"
      },
      "outputs": [],
      "source": [
        "# Contenido de requirements.txt\n",
        "\n",
        "\"\"\"\n",
        "streamlit==1.32.0\n",
        "requests==2.31.0\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6YIqfv4SFKs"
      },
      "source": [
        "### Construir la Imagen\n",
        "\n",
        "Desde tu directorio del proyecto, ejecutá:\n",
        "\n",
        "```bash\n",
        "docker build -t asistente-nlp-frontend:v1.0 .\n",
        "```\n",
        "\n",
        "**¿Qué significa cada parte?**\n",
        "- `docker build`: Comando para construir una imagen\n",
        "- `-t asistente-nlp-frontend:v1.0`: Tag (nombre y versión) de la imagen\n",
        "- `.`: Contexto de build (directorio actual)\n",
        "\n",
        "El proceso puede tardar unos minutos. Docker va a:\n",
        "1. Descargar la imagen base Python\n",
        "2. Copiar tu código\n",
        "3. Instalar las dependencias\n",
        "4. Crear la imagen final\n",
        "\n",
        "### Verificar la Imagen\n",
        "\n",
        "```bash\n",
        "docker images\n",
        "```\n",
        "\n",
        "Deberías ver:\n",
        "```\n",
        "REPOSITORY                  TAG       SIZE\n",
        "asistente-nlp-frontend      v1.0      450MB\n",
        "```\n",
        "\n",
        "### Actualizar docker-compose.yml\n",
        "\n",
        "Agregá tu nueva imagen al docker-compose existente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGGySv1XSFKs"
      },
      "outputs": [],
      "source": [
        "# Agregar al final de docker-compose.yml (después del servicio ollama)\n",
        "\n",
        "\"\"\"\n",
        "  streamlit-app:\n",
        "    image: asistente-nlp-frontend:v1.0\n",
        "    container_name: asistente-frontend\n",
        "    restart: unless-stopped\n",
        "    ports:\n",
        "      - \"8501:8501\"\n",
        "    networks:\n",
        "      - n8n-network\n",
        "    depends_on:\n",
        "      - n8n\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JSsR0S0SFKs"
      },
      "source": [
        "Ahora reiniciá los servicios:\n",
        "\n",
        "```bash\n",
        "docker-compose up -d\n",
        "```\n",
        "\n",
        "Tenés tres servicios corriendo:\n",
        "- **Ollama** en puerto 11434\n",
        "- **n8n** en puerto 5678  \n",
        "- **Streamlit** en puerto 8501\n",
        "\n",
        "### Compartir tu Imagen\n",
        "\n",
        "**Opción A: Guardar como archivo**\n",
        "\n",
        "```bash\n",
        "# Exportar imagen a archivo .tar\n",
        "docker save asistente-nlp-frontend:v1.0 -o asistente-nlp-frontend.tar\n",
        "\n",
        "# Comprimir (opcional)\n",
        "gzip asistente-nlp-frontend.tar\n",
        "```\n",
        "\n",
        "Ahora podés compartir el archivo `.tar.gz`. Para importarlo en otra máquina:\n",
        "\n",
        "```bash\n",
        "docker load -i asistente-nlp-frontend.tar\n",
        "```\n",
        "\n",
        "**Opción B: Subir a Docker Hub**\n",
        "\n",
        "Docker Hub es un registro público de imágenes (como GitHub para código).\n",
        "\n",
        "```bash\n",
        "# 1. Crear cuenta en hub.docker.com (gratis)\n",
        "\n",
        "# 2. Login desde terminal\n",
        "docker login\n",
        "\n",
        "# 3. Taggear con tu usuario\n",
        "docker tag asistente-nlp-frontend:v1.0 tuusuario/asistente-nlp-frontend:v1.0\n",
        "\n",
        "# 4. Subir a Docker Hub\n",
        "docker push tuusuario/asistente-nlp-frontend:v1.0\n",
        "```\n",
        "\n",
        "Ahora cualquiera puede descargar tu imagen con:\n",
        "```bash\n",
        "docker pull tuusuario/asistente-nlp-frontend:v1.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlRlvclNSFKs"
      },
      "source": [
        "---\n",
        "\n",
        "## Paso 8: Despliegue en Servidores\n",
        "\n",
        "Docker se usa masivamente en servidores y en la nube. Esta sección te introduce a los conceptos de deployment en producción.\n",
        "\n",
        "### ¿Por qué Desplegar en un Servidor?\n",
        "\n",
        "Hasta ahora tu aplicación corre en tu máquina local. Para que otros puedan usarla necesitás:\n",
        "\n",
        "- **Disponibilidad 24/7:** Servidores que no se apagan\n",
        "- **Acceso público:** Dirección IP pública o dominio\n",
        "- **Recursos:** Servidores con más capacidad\n",
        "- **Confiabilidad:** Infraestructura con backups y redundancia\n",
        "\n",
        "### Docker en Servidores\n",
        "\n",
        "Docker es extremadamente popular en servidores porque:\n",
        "\n",
        "1. **Simplicidad:** El mismo `docker-compose.yml` funciona en tu laptop y en el servidor\n",
        "2. **Aislamiento:** Múltiples aplicaciones pueden correr sin interferir entre sí\n",
        "3. **Escalabilidad:** Podés replicar contenedores fácilmente\n",
        "4. **Portabilidad:** Migrás entre providers sin cambiar nada\n",
        "\n",
        "### Opciones de Hosting\n",
        "\n",
        "#### Proveedores Cloud Principales\n",
        "\n",
        "**1. DigitalOcean (Recomendado para empezar)**\n",
        "- **Droplets:** Servidores virtuales simples\n",
        "- **Costo:** Desde $6/mes\n",
        "- **Ventaja:** Muy fácil de usar, buena documentación\n",
        "- **Ideal para:** Proyectos pequeños, aprendizaje\n",
        "\n",
        "**2. Amazon Web Services (AWS)**\n",
        "- **EC2:** Máquinas virtuales con Docker\n",
        "- **Lightsail:** Opción más simple (similar a DigitalOcean)\n",
        "- **Costo:** Desde $3.50/mes (Lightsail)\n",
        "- **Ventaja:** Escalabilidad casi infinita\n",
        "- **Desventaja:** Curva de aprendizaje empinada\n",
        "\n",
        "**3. Google Cloud Platform (GCP)**\n",
        "- **Compute Engine:** VMs con Docker\n",
        "- **Cloud Run:** Deploy directo de contenedores (serverless)\n",
        "- **Ventaja:** Integración con servicios de Google\n",
        "\n",
        "**4. Railway (Recomendado para estudiantes)**\n",
        "- **Ventaja:** $5 gratis por mes para estudiantes\n",
        "- **Deploy automático** desde GitHub\n",
        "- **Ideal para:** Prototipos, proyectos educativos\n",
        "\n",
        "### Proceso General de Despliegue\n",
        "\n",
        "El proceso es similar en todos los providers:\n",
        "\n",
        "**1. Crear un servidor**\n",
        "```bash\n",
        "# En el provider elegido:\n",
        "# - Seleccionar Ubuntu 22.04 LTS\n",
        "# - Elegir plan (mínimo 2GB RAM)\n",
        "# - Configurar SSH key\n",
        "# - Crear servidor\n",
        "```\n",
        "\n",
        "**2. Conectarse por SSH**\n",
        "```bash\n",
        "ssh root@tu-servidor-ip\n",
        "```\n",
        "\n",
        "**3. Instalar Docker**\n",
        "```bash\n",
        "# Actualizar sistema\n",
        "apt update && apt upgrade -y\n",
        "\n",
        "# Instalar Docker\n",
        "curl -fsSL https://get.docker.com -o get-docker.sh\n",
        "sh get-docker.sh\n",
        "\n",
        "# Instalar Docker Compose\n",
        "apt install docker-compose -y\n",
        "```\n",
        "\n",
        "**4. Subir tu proyecto**\n",
        "```bash\n",
        "# Opción A: Clonar desde Git (recomendado)\n",
        "git clone https://github.com/tuusuario/asistente-nlp.git\n",
        "cd asistente-nlp\n",
        "\n",
        "# Opción B: Copiar archivos con SCP desde tu máquina\n",
        "# scp -r /ruta/local/proyecto root@servidor-ip:/root/\n",
        "```\n",
        "\n",
        "**5. Levantar servicios**\n",
        "```bash\n",
        "docker-compose up -d\n",
        "```\n",
        "\n",
        "**6. Configurar firewall**\n",
        "```bash\n",
        "# Permitir puertos necesarios\n",
        "ufw allow 5678/tcp   # n8n\n",
        "ufw allow 8501/tcp   # Streamlit\n",
        "ufw allow 22/tcp     # SSH\n",
        "ufw enable\n",
        "```\n",
        "\n",
        "**7. Acceder a tu aplicación**\n",
        "```\n",
        "http://tu-servidor-ip:8501  (Streamlit)\n",
        "http://tu-servidor-ip:5678  (n8n)\n",
        "```\n",
        "\n",
        "### Consideraciones de Producción\n",
        "\n",
        "Para un deployment serio considerá:\n",
        "\n",
        "**Seguridad:**\n",
        "- Cambiar puertos por defecto\n",
        "- Usar variables de entorno para secrets\n",
        "- Configurar SSL/TLS (HTTPS)\n",
        "- Actualizar regularmente\n",
        "- Configurar fail2ban (protección contra ataques)\n",
        "\n",
        "**Disponibilidad:**\n",
        "- Configurar backups automáticos\n",
        "- Monitoreo de servicios\n",
        "- Logs centralizados\n",
        "- Alertas ante caídas\n",
        "\n",
        "**Rendimiento:**\n",
        "- Usar un reverse proxy (Nginx)\n",
        "- Configurar límites de recursos\n",
        "- Caché cuando sea apropiado\n",
        "- CDN para contenido estático\n",
        "\n",
        "### Costos Estimados\n",
        "\n",
        "Para este proyecto:\n",
        "\n",
        "**Opción Básica:**\n",
        "- DigitalOcean Droplet 2GB: $12/mes\n",
        "- Dominio: $10-15/año\n",
        "- SSL: Gratis (Let's Encrypt)\n",
        "- **Total:** ~$13-15/mes\n",
        "\n",
        "**Opción Estudiante:**\n",
        "- Railway: $5 crédito/mes (gratis con GitHub Student Pack)\n",
        "- **Total:** Gratis durante estudios\n",
        "\n",
        "### Próximos Pasos en Deployment\n",
        "\n",
        "Para profundizar:\n",
        "\n",
        "1. **Dominio personalizado:** En lugar de IP, usar `asistente-nlp.tudominio.com`\n",
        "2. **HTTPS:** Configurar SSL con Let's Encrypt\n",
        "3. **CI/CD:** Deploy automático cuando hacés push a Git\n",
        "4. **Kubernetes:** Para proyectos grandes que necesitan escalar\n",
        "5. **Monitoring:** Grafana + Prometheus para observabilidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp0VbZkeSFKt"
      },
      "source": [
        "---\n",
        "\n",
        "## Solución de Problemas Comunes\n",
        "\n",
        "### Docker no inicia\n",
        "\n",
        "**Síntoma:** Errores al ejecutar comandos Docker\n",
        "\n",
        "**Solución:**\n",
        "1. Verificar que Docker Desktop esté corriendo\n",
        "2. En Windows, reiniciar el servicio Docker\n",
        "3. En Linux: `sudo systemctl restart docker`\n",
        "\n",
        "### Puerto en uso\n",
        "\n",
        "**Síntoma:** Error \"port is already allocated\"\n",
        "\n",
        "**Solución:**\n",
        "```bash\n",
        "# Ver qué está usando el puerto (ej: 5678)\n",
        "# Windows:\n",
        "netstat -ano | findstr :5678\n",
        "\n",
        "# Mac/Linux:\n",
        "lsof -i :5678\n",
        "\n",
        "# Matar el proceso o cambiar puerto en docker-compose.yml\n",
        "```\n",
        "\n",
        "### n8n no se conecta con Ollama\n",
        "\n",
        "**Síntoma:** Timeout o error de conexión\n",
        "\n",
        "**Solución:**\n",
        "1. Verificar que ambos contenedores estén UP: `docker-compose ps`\n",
        "2. Verificar la URL en el nodo HTTP Request: debe ser `http://ollama:11434` (NO localhost)\n",
        "3. Verificar que estén en la misma red: `docker network inspect nombreproyecto_n8n-network`\n",
        "\n",
        "### Modelo muy lento\n",
        "\n",
        "**Síntoma:** Respuestas tardan minutos\n",
        "\n",
        "**Solución:**\n",
        "1. Verificar uso de recursos: `docker stats`\n",
        "2. Si tenés poca RAM, usar un modelo más chico (granite4:micro)\n",
        "3. Cerrar otras aplicaciones pesadas\n",
        "4. Aumentar timeout en el nodo HTTP Request\n",
        "\n",
        "### Sin espacio en disco\n",
        "\n",
        "**Síntoma:** \"no space left on device\"\n",
        "\n",
        "**Solución:**\n",
        "```bash\n",
        "# Limpiar imágenes y contenedores sin usar\n",
        "docker system prune -a\n",
        "\n",
        "# Eliminar modelos de Ollama que no uses\n",
        "docker exec -it ollama ollama rm nombre_modelo\n",
        "```\n",
        "\n",
        "### Perdí mis workflows\n",
        "\n",
        "**Síntoma:** Después de `docker-compose down -v` se perdió todo\n",
        "\n",
        "**Prevención:**\n",
        "- NUNCA uses `-v` a menos que quieras borrar datos\n",
        "- Exportá workflows regularmente desde n8n\n",
        "- Hacé backups del volumen:\n",
        "```bash\n",
        "docker run --rm -v nombreproyecto_n8n_data:/data -v $(pwd):/backup \\\n",
        "  ubuntu tar czf /backup/n8n_backup.tar.gz /data\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLaOtM83SFKt"
      },
      "source": [
        "---\n",
        "\n",
        "## Ejercicios Prácticos\n",
        "\n",
        "### Ejercicio 1: Experimentación con Modelos\n",
        "\n",
        "**Objetivo:** Comparar diferentes tamaños de Granite 4\n",
        "\n",
        "1. Descargá granite4:micro, granite4:tiny\n",
        "2. Probá la misma consulta con ambos\n",
        "3. Medí tiempo de respuesta y calidad\n",
        "4. Documentá tus conclusiones\n",
        "\n",
        "### Ejercicio 2: Extender el Workflow\n",
        "\n",
        "**Objetivo:** Agregar nueva tarea de NLP\n",
        "\n",
        "Implementá soporte para **Corrección gramatical**:\n",
        "1. Editá el nodo \"Detectar Tarea NLP\"\n",
        "2. Agregá detección para \"corrige\" o \"corrección\"\n",
        "3. Creá un prompt especializado\n",
        "4. Probá con texto con errores\n",
        "\n",
        "### Ejercicio 3: Mejorar la Interfaz\n",
        "\n",
        "**Objetivo:** Agregar funcionalidades a Streamlit\n",
        "\n",
        "1. Implementá historial de consultas\n",
        "2. Agregá estadísticas de uso\n",
        "3. Permití exportar resultados a TXT/JSON\n",
        "\n",
        "### Ejercicio 4: Despliegue Local Completo\n",
        "\n",
        "**Objetivo:** Crear sistema completamente funcional\n",
        "\n",
        "1. Creá tu imagen Docker personalizada\n",
        "2. Actualizá docker-compose.yml con todos los servicios\n",
        "3. Documentá el proceso de instalación\n",
        "4. Compartí la imagen con un compañero\n",
        "\n",
        "### Proyecto Final: Sistema Multiagente\n",
        "\n",
        "**Objetivo:** Implementar arquitectura avanzada\n",
        "\n",
        "Creá múltiples workflows especializados:\n",
        "1. Workflow coordinador que decide qué workflow llamar\n",
        "2. Workflow especializado en NER\n",
        "3. Workflow especializado en análisis de sentimiento\n",
        "4. Workflow que combina resultados de múltiples análisis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93OSRN_vSFKt"
      },
      "source": [
        "---\n",
        "\n",
        "## Comandos de Referencia Rápida\n",
        "\n",
        "### Docker Compose\n",
        "```bash\n",
        "docker-compose up -d          # Levantar servicios\n",
        "docker-compose down           # Detener y eliminar contenedores\n",
        "docker-compose ps             # Ver estado de servicios\n",
        "docker-compose logs -f        # Ver logs en tiempo real\n",
        "docker-compose restart        # Reiniciar servicios\n",
        "```\n",
        "\n",
        "### Docker Básico\n",
        "```bash\n",
        "docker ps                     # Contenedores corriendo\n",
        "docker ps -a                  # Todos los contenedores\n",
        "docker images                 # Listar imágenes\n",
        "docker logs nombre            # Ver logs de un contenedor\n",
        "docker exec -it nombre bash   # Entrar a un contenedor\n",
        "docker stats                  # Ver uso de recursos\n",
        "```\n",
        "\n",
        "### Ollama\n",
        "```bash\n",
        "docker exec -it ollama ollama list         # Listar modelos\n",
        "docker exec -it ollama ollama pull modelo  # Descargar modelo\n",
        "docker exec -it ollama ollama rm modelo    # Eliminar modelo\n",
        "docker exec -it ollama ollama run modelo   # Ejecutar interactivamente\n",
        "```\n",
        "\n",
        "### Limpieza\n",
        "```bash\n",
        "docker system prune           # Limpiar recursos sin usar\n",
        "docker system prune -a        # Limpiar TODO sin usar\n",
        "docker volume prune           # Limpiar volúmenes sin usar\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_UEKgCiSFKt"
      },
      "source": [
        "---\n",
        "\n",
        "## Conclusiones y Próximos Pasos\n",
        "\n",
        "### Lo que Aprendiste\n",
        "\n",
        "En este laboratorio desarrollaste habilidades fundamentales:\n",
        "\n",
        "1. **Fundamentos de Docker:** Contenedores, imágenes, volúmenes, redes\n",
        "2. **Docker Compose:** Orquestación de servicios multi-contenedor\n",
        "3. **n8n:** Automatización visual de workflows\n",
        "4. **Ollama:** Ejecución local de modelos de lenguaje\n",
        "5. **Granite 4:** Modelo de lenguaje híbrido de última generación\n",
        "6. **Construcción de imágenes:** Dockerfiles y distribución\n",
        "7. **Conceptos de deployment:** Despliegue en servidores\n",
        "\n",
        "### Aplicabilidad Profesional\n",
        "\n",
        "Estas habilidades son altamente demandadas en:\n",
        "- DevOps Engineer\n",
        "- MLOps Engineer\n",
        "- Backend Developer\n",
        "- Data Engineer\n",
        "- NLP Engineer\n",
        "\n",
        "### Recursos para Profundizar\n",
        "\n",
        "**Docker:**\n",
        "- Documentación oficial: https://docs.docker.com/\n",
        "- Docker Curriculum: https://docker-curriculum.com/\n",
        "\n",
        "**n8n:**\n",
        "- Documentación: https://docs.n8n.io/\n",
        "- Templates de workflows: https://n8n.io/workflows/\n",
        "\n",
        "**Ollama:**\n",
        "- Catálogo de modelos: https://ollama.ai/library\n",
        "- GitHub: https://github.com/ollama/ollama\n",
        "\n",
        "### Preguntas de Reflexión\n",
        "\n",
        "Antes de finalizar, reflexioná sobre:\n",
        "\n",
        "1. **LLMs locales vs APIs:** ¿Cuándo preferirías cada opción en un proyecto real?\n",
        "2. **Arquitectura:** ¿Cómo escalarías este sistema para 100 usuarios simultáneos?\n",
        "3. **Seguridad:** ¿Qué vulnerabilidades identificás en el sistema actual?\n",
        "4. **Costos:** ¿Qué sería más económico: Ollama local o usar APIs de OpenAI/Anthropic?\n",
        "\n",
        "---\n",
        "\n",
        "**Última actualización:** Noviembre 2025  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}