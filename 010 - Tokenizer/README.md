# Procesamiento de Lenguaje Natural con Transformers y Modelos de Lenguaje

Este repositorio contiene una colecci贸n de notebooks orientados a introducir, explorar y aplicar los conceptos fundamentales y avanzados del procesamiento de lenguaje natural (NLP) mediante el uso de modelos de lenguaje y arquitecturas basadas en transformers.

Los contenidos se organizan de forma progresiva, desde la motivaci贸n y estructura de los modelos de lenguaje, hasta aplicaciones concretas como generaci贸n de texto y resumen autom谩tico.

---

##  Estructura del repositorio

### 1. Fundamentos de los Modelos de Lenguaje
- **01_Introduccion_Modelos_Lenguaje.ipynb**  
  Introducci贸n al concepto de modelos de lenguaje, historia, evoluci贸n, y motivaciones te贸ricas. Se abordan temas como la modelizaci贸n probabil铆stica del lenguaje y los primeros modelos estad铆sticos.

- **02_Tokens_y_Embeddings.ipynb**  
  Explicaci贸n detallada del proceso de tokenizaci贸n (WordPiece, Byte Pair Encoding, etc.) y las representaciones vectoriales (embeddings) como Word2Vec, GloVe, y contextualizados (BERT, GPT).

### 2. Arquitectura Transformer y Aplicaciones
- **03_Arquitectura_Transformer.ipynb**  
  Descripci贸n del modelo Transformer propuesto por Vaswani et al. Se explican conceptos clave como atenci贸n, multi-head attention, codificadores-decodificadores, y auto-atenci贸n.

- **04_Aplicaciones_Transformers.ipynb**  
  Casos pr谩cticos con Transformers preentrenados aplicados a tareas como clasificaci贸n, traducci贸n, reconocimiento de entidades y m谩s.

### 3. Aplicaciones Avanzadas
- **05_Generacion_Texto_Avanzada.ipynb**  
  Exploraci贸n de t茅cnicas de generaci贸n de texto (greedy, beam search, top-k, nucleus sampling), y su implementaci贸n con modelos como GPT y T5.

- **06_Sumarizacion_Aplicada.ipynb**  
  Aplicaci贸n de modelos preentrenados para la tarea de sumarizaci贸n autom谩tica de textos extensos. Se incluyen comparaciones entre extractive y abstractive summarization.

### 4. Recursos Complementarios
- **Guia_Token_HuggingFace.md**  
  Gu铆a paso a paso para generar, almacenar y utilizar tu token de autenticaci贸n personal de Hugging Face para acceder a modelos, datasets o usar APIs en notebooks.

- **Guia_Token_HuggingFace.ipynb**  
  Versi贸n interactiva en notebook que implementa los pasos de la gu铆a anterior directamente desde Python.

- **intro-llms-texto-con-texto.ipynb**  
  Demostraci贸n pr谩ctica de c贸mo los LLMs pueden utilizarse para tareas complejas a partir de instrucciones textuales. Ejemplos con transformers aplicados directamente a problemas reales.

- **tlon-uqbar-orbis-tertius.txt**  
  Texto de referencia literaria ("Tl枚n, Uqbar, Orbis Tertius" de Borges), utilizado en ejemplos de procesamiento y generaci贸n de texto. til para tareas de inferencia sem谩ntica, an谩lisis de estilo, y generaci贸n basada en corpus literario.

---

##  Objetivos del proyecto

- Comprender el funcionamiento interno de los modelos de lenguaje y su evoluci贸n.
- Aprender a aplicar modelos preentrenados de Hugging Face a diversas tareas de NLP.
- Desarrollar habilidades pr谩cticas en generaci贸n de texto, clasificaci贸n, y resumen autom谩tico.
- Reflexionar sobre el impacto de estos modelos en la representaci贸n del conocimiento textual.

---

## 锔 Requisitos t茅cnicos

- Python 3.8+
- `transformers`
- `huggingface_hub`
- `datasets`
- `torch` o `tensorflow` (seg煤n el modelo)
- `gradio` (opcional para visualizaciones interactivas)

Instalaci贸n recomendada:

```bash
pip install -U transformers huggingface_hub datasets gradio
